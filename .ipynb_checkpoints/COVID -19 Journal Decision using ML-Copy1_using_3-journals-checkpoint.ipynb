{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Journal Classification using Machine Learning Techniques<br>\n",
    "## Team Members:<br>\n",
    "1.Yerramaddu Jahnavi - 181CO260 <br>\n",
    "2.Swathi J S - 181CO155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of our system is to classify the abstracts related to covid with their respective journals so that a researcher can refer to articles of his interest from the required journals instead of searching all the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import future\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description | Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source_x'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['journal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (data['journal'] =='bioRxiv') | (data['journal'] == 'PLoS One') | (data['journal'] == 'BMJ')\n",
    "updated_df = data.loc[filt,['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id', 'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id']]\n",
    "updated_df.to_csv('./data3.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered=pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['journal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "language = []\n",
    "for i in data_filtered['title']:\n",
    "    language.append(detect(i))\n",
    "data_filtered['Title Language']=language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['Title Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['abstract'] = data_filtered['abstract'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = []\n",
    "for i in data_filtered['abstract']:\n",
    "    language.append(detect(i))\n",
    "data_filtered['Abstract Language']=language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['Abstract Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (data_filtered['Abstract Language'] =='en')\n",
    "updated_df = data_filtered.loc[filt,data_filtered.columns]\n",
    "updated_df.to_csv('./data4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered = pd.read_csv(\"data4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered['Title Language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Recognisation Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "ner = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag text\n",
    "txt = english_data_filtered[\"abstract\"].iloc[1]\n",
    "doc = ner(txt)\n",
    "## display result\n",
    "spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag text and exctract tags into a list\n",
    "english_data_filtered[\"tags\"] = english_data_filtered[\"abstract\"].apply(lambda x: [(tag.text, tag.label_) for tag in ner(x).ents] )\n",
    "## utils function to count the element of a list\n",
    "def utils_lst_count(lst):\n",
    "    dic_counter = collections.Counter()\n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    lst_count = [ {key:value} for key,value in dic_counter.items() ]\n",
    "    return lst_count\n",
    "\n",
    "## count tags\n",
    "english_data_filtered[\"tags\"] = english_data_filtered[\"tags\"].apply(lambda x: utils_lst_count(x))\n",
    "\n",
    "## utils function create new column for each tag category\n",
    "def utils_ner_features(lst_dics_tuples, tag):\n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "## extract features\n",
    "tags_set = []\n",
    "for lst in english_data_filtered[\"tags\"].tolist():\n",
    "     for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "tags_set = list(set(tags_set))\n",
    "for feature in tags_set:\n",
    "    english_data_filtered[\"tags_\"+feature] = english_data_filtered[\"tags\"].apply(lambda x: utils_ner_features(x, feature))\n",
    "\n",
    "## print result\n",
    "english_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered['tags_CARDINAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def CleanText(raw_text, remove_stopwords=True, stemming= False, flg_lemm=True, split_text=True):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "        #stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        words = [lem.lemmatize(word) for word in words]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_abstract=[]\n",
    "for i in english_data_filtered['abstract']:\n",
    "    cleaned_abstract.append(CleanText(i))\n",
    "english_data_filtered['Cleaned Abstract']=cleaned_abstract\n",
    "english_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered['Cleaned Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = []\n",
    "for i in english_data_filtered['Cleaned Abstract']:\n",
    "    sentence = \" \"\n",
    "    new = \" \"\n",
    "    for j in i:\n",
    "        sentence += (j + new)\n",
    "    sen.append(sentence)\n",
    "english_data_filtered['Cleaned Abstract']=sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLoS One - Most Frequent Tags in \"PLoS One\" Journal Abstract\n",
    "import seaborn as sns\n",
    "tags_list = english_data_filtered[english_data_filtered[\"journal\"]==\"PLoS One\"][\"tags\"].sum()\n",
    "map_lst = list(map(lambda x: list(x.keys())[0], tags_list))\n",
    "english_data_filtered_tags = pd.DataFrame(map_lst, columns=['tag','type'])\n",
    "english_data_filtered_tags[\"count\"] = 1\n",
    "english_data_filtered_tags = english_data_filtered_tags.groupby(['type',  \n",
    "                'tag']).count().reset_index().sort_values(\"count\", \n",
    "                 ascending=False)\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent tags\", fontsize=12)\n",
    "sns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n",
    "            data=english_data_filtered_tags.iloc[:10,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bioRxiv - Most Frequent Tags in \"bioRxiv\" Journal Abstract\n",
    "import seaborn as sns\n",
    "tags_list = english_data_filtered[english_data_filtered[\"journal\"]==\"bioRxiv\"][\"tags\"].sum()\n",
    "map_lst = list(map(lambda x: list(x.keys())[0], tags_list))\n",
    "english_data_filtered_tags = pd.DataFrame(map_lst, columns=['tag','type'])\n",
    "english_data_filtered_tags[\"count\"] = 1\n",
    "english_data_filtered_tags = english_data_filtered_tags.groupby(['type',  \n",
    "                'tag']).count().reset_index().sort_values(\"count\", \n",
    "                 ascending=False)\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent tags\", fontsize=12)\n",
    "sns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n",
    "            data=english_data_filtered_tags.iloc[:10,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nature - Most Frequent Tags in \"Nature\" Journal Abstract\n",
    "import seaborn as sns\n",
    "tags_list = english_data_filtered[english_data_filtered[\"journal\"]==\"BMJ\"][\"tags\"].sum()\n",
    "map_lst = list(map(lambda x: list(x.keys())[0], tags_list))\n",
    "english_data_filtered_tags = pd.DataFrame(map_lst, columns=['tag','type'])\n",
    "english_data_filtered_tags[\"count\"] = 1\n",
    "english_data_filtered_tags = english_data_filtered_tags.groupby(['type',  \n",
    "                'tag']).count().reset_index().sort_values(\"count\", \n",
    "                 ascending=False)\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent tags\", fontsize=12)\n",
    "sns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n",
    "            data=english_data_filtered_tags.iloc[:10,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered['word_count'] = english_data_filtered[\"Cleaned Abstract\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "english_data_filtered['char_count'] = english_data_filtered[\"Cleaned Abstract\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "english_data_filtered['sentence_count'] = english_data_filtered[\"Cleaned Abstract\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "english_data_filtered['avg_word_length'] = english_data_filtered['char_count'] / english_data_filtered['word_count']\n",
    "english_data_filtered['avg_sentence_length'] = english_data_filtered['word_count'] / english_data_filtered['sentence_count']\n",
    "english_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x, y = \"char_count\", \"journal\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(x, fontsize=12)\n",
    "for i in english_data_filtered[y].unique():\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=True, kde=False, \n",
    "                 bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                 axlabel=\"histogram\", ax=ax[0])\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=False, kde=True, \n",
    "                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                 ax=ax[1])\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(english_data_filtered[y].unique())\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = \"word_count\", \"journal\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(x, fontsize=12)\n",
    "for i in english_data_filtered[y].unique():\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=True, kde=False, \n",
    "                 bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                 axlabel=\"histogram\", ax=ax[0])\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=False, kde=True, \n",
    "                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                 ax=ax[1])\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(english_data_filtered[y].unique())\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = \"avg_word_length\", \"journal\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(x, fontsize=12)\n",
    "for i in english_data_filtered[y].unique():\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=True, kde=False, \n",
    "                 bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                 axlabel=\"histogram\", ax=ax[0])\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=False, kde=True, \n",
    "                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                 ax=ax[1])\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(english_data_filtered[y].unique())\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = \"avg_sentence_length\", \"journal\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(x, fontsize=12)\n",
    "for i in english_data_filtered[y].unique():\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=True, kde=False, \n",
    "                 bins=10, hist_kws={\"alpha\":0.8}, \n",
    "                 axlabel=\"histogram\", ax=ax[0])\n",
    "    sns.distplot(english_data_filtered[english_data_filtered[y]==i][x], hist=False, kde=True, \n",
    "                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n",
    "                 ax=ax[1])\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(english_data_filtered[y].unique())\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "#PLoS One\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"PLoS One\"][\"Cleaned Abstract\"]\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "    \n",
    "## unigrams\n",
    "dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "english_data_filtered_uni_PLoSOne = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                       columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_uni_PLoSOne.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                  legend=False).grid(axis='x')\n",
    "ax[0].set(ylabel=None)\n",
    "    \n",
    "## bigrams\n",
    "dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "english_data_filtered_bi_PLoSOne = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                      columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_bi_PLoSOne[\"Word\"] = english_data_filtered_bi_PLoSOne[\"Word\"].apply(lambda x: \" \".join(\n",
    "                   string for string in x) )\n",
    "english_data_filtered_bi_PLoSOne.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                  legend=False).grid(axis='x')\n",
    "ax[1].set(ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BMJ\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"BMJ\"][\"Cleaned Abstract\"]\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "    \n",
    "## unigrams\n",
    "dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "english_data_filtered_uni_BMJ = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                       columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_uni_BMJ.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                  legend=False).grid(axis='x')\n",
    "ax[0].set(ylabel=None)\n",
    "    \n",
    "## bigrams\n",
    "dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "english_data_filtered_bi_BMJ = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                      columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_bi_BMJ[\"Word\"] = english_data_filtered_bi_BMJ[\"Word\"].apply(lambda x: \" \".join(\n",
    "                   string for string in x) )\n",
    "english_data_filtered_bi_BMJ.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                  legend=False).grid(axis='x')\n",
    "ax[1].set(ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bioRxiv\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"bioRxiv\"][\"Cleaned Abstract\"]\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "    \n",
    "## unigrams\n",
    "dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "english_data_filtered_uni_bioRxiv = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                       columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_uni_bioRxiv.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                  legend=False).grid(axis='x')\n",
    "ax[0].set(ylabel=None)\n",
    "    \n",
    "## bigrams\n",
    "dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "english_data_filtered_bi_bioRxiv = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                      columns=[\"Word\",\"Freq\"])\n",
    "english_data_filtered_bi_bioRxiv[\"Word\"] = english_data_filtered_bi_bioRxiv[\"Word\"].apply(lambda x: \" \".join(\n",
    "                   string for string in x) )\n",
    "english_data_filtered_bi_bioRxiv.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                  legend=False).grid(axis='x')\n",
    "ax[1].set(ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(background_color='black', max_words=100, \n",
    "                         max_font_size=35)\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"bioRxiv\"][\"Cleaned Abstract\"]\n",
    "wc_bioRxiv = wc.generate(str(corpus))\n",
    "fig = plt.figure(num=1)\n",
    "plt.axis('off')\n",
    "plt.imshow(wc, cmap=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black', max_words=100, \n",
    "                         max_font_size=35)\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"BMJ\"][\"Cleaned Abstract\"]\n",
    "wc_BMJ = wc.generate(str(corpus))\n",
    "fig = plt.figure(num=1)\n",
    "plt.axis('off')\n",
    "plt.imshow(wc, cmap=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black', max_words=100, \n",
    "                         max_font_size=35)\n",
    "corpus = english_data_filtered[english_data_filtered[\"journal\"]==\"PLoS One\"][\"Cleaned Abstract\"]\n",
    "wc_PLoSOne = wc.generate(str(corpus))\n",
    "fig = plt.figure(num=1)\n",
    "plt.axis('off')\n",
    "plt.imshow(wc, cmap=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf= english_data_filtered.drop(['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id','license', 'abstract', 'publish_time', 'authors','mag_id','who_covidence_id','arxiv_id','pdf_json_files','pmc_json_files','url','s2_id','Title Language'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dtf['y'] = le.fit_transform(dtf['journal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf['journal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf=dtf.drop(['sentence_count'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf=dtf.drop(['char_count'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "english_data_filtered_bi_bioRxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf weight is composed by two terms: The first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.<br>\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)<br>\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "abstract_text = list(dtf['Cleaned Abstract'].values)\n",
    "vect_word = TfidfVectorizer(max_features=10000, analyzer='word', stop_words='english', ngram_range=(1,2), dtype=np.float32) \n",
    "vect_word.fit(abstract_text)\n",
    "tfidf_complete = vect_word.transform(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of features : %d \\n\" %len(vect_word.get_feature_names()))\n",
    "print (\"Show some feature names : \\n\", vect_word.get_feature_names()[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(zip(vect_word.get_feature_names(), vect_word.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf_complete']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf_complete']\n",
    "tfidf.sort_values(by=['tfidf_complete'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.spy(tfidf_complete,markersize=0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split( dtf['Cleaned Abstract'], dtf['y'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=10000, analyzer='word', stop_words='english', ngram_range=(1,2), dtype=np.float32)\n",
    "train_tfidf = vect_word.fit_transform(x_train)\n",
    "test_tfidf = vect_word.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "def select_features(train_X, train_y, test_X, k):\n",
    "    if k == 'all':\n",
    "        return train_X, test_X\n",
    "\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    selector.fit(train_X, train_y)\n",
    "    train_X = selector.transform(train_X)\n",
    "    test_X = selector.transform(test_X)\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_final,tfidf_test_final = select_features(train_tfidf,y_train,test_tfidf,627) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,100))\n",
    "plt.spy(tfidf_train_final,markersize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC(multi_class='ovr')\n",
    "linear_svc.fit(tfidf_train_final,y_train)\n",
    "predictions = linear_svc.predict(tfidf_test_final)\n",
    "print (\"Accuracy of this SVM = \" + str(metrics.accuracy_score(y_test, predictions)))\n",
    "print (\"Confusion matrix = \" + str(metrics.confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors = 5)\n",
    "model.fit(tfidf_train_final,y_train)\n",
    "predictions = model.predict(tfidf_test_final)\n",
    "print (\"Accuracy of this KNN = \" + str(metrics.accuracy_score(y_test, predictions)))\n",
    "print (\"Confusion matrix = \" + str(metrics.confusion_matrix(y_test, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rates=[]\n",
    "for i in np.arange(1, 100):\n",
    "    new_model = KNeighborsClassifier(n_neighbors = i)\n",
    "    new_model.fit(tfidf_train_final,y_train)\n",
    "    new_predictions = new_model.predict(tfidf_test_final)\n",
    "    error_rates.append(np.mean(new_predictions != y_test))\n",
    "plt.plot(error_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 18)\n",
    "model.fit(tfidf_train_final,y_train)\n",
    "predictions = model.predict(tfidf_test_final)\n",
    "print (\"Accuracy of this KNN = \" + str(metrics.accuracy_score(y_test, predictions)))\n",
    "print (\"Confusion matrix = \" + str(metrics.confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(tfidf_train_final,y_train)\n",
    "scores = cross_val_score(xgbc, tfidf_train_final, y_train, cv=5)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgbc.predict(tfidf_test_final)\n",
    "print (\"Accuracy of this XGBoost = \" + str(metrics.accuracy_score(y_test, predictions)))\n",
    "print (\"Confusion matrix = \" + str(metrics.confusion_matrix(y_test, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
